# Literature Review: Mitigating Hallucinations Induced by Biases in Large Language Models

This repository contains a literature review by Filip Dimitrieviski and Akram Moustafa (April 2025) to explore the relationship between hallucinations and biases in Large Language Models (LLMs).

## Highlights
- Analyzes recent research on hallucinations in LLMs and the complexity of mitigation.
- Highlights biases as a hidden source of hallucinations.
- Identifies a research gap: lack of studies proposing strategies to mitigate hallucinations induced by biases.
- Provides a comprehensive review of bias-aware mitigation approaches.

## Contents
- `literature_review.tex`: LaTeX source of the paper
- `refs.bib`: Reference file with citations
- `LLM_Hallucination_Bias_Review.pdf`: Compiled PDF of the paper

## Authors
- Filip Dimitrieviski
- Akram Moustafa

## Future Work
- Continue research on bias-induced hallucinations in LLMs.
- Explore practical mitigation strategies and benchmark performance.
